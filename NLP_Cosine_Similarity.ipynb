{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBV-Yc9r_59m",
        "outputId": "553bcf88-c5e4-4740-8543-8e86822d2f3a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mLcfXzk8477",
        "outputId": "c049edfc-c90f-4eda-8172-3164caeb5e80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarity Without Embedding:\n",
            "[[1.         0.42588623]\n",
            " [0.42588623 1.        ]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Documents\n",
        "doc1 = \"\"\"A computer is a complex electronic device designed to process and store data. It operates on the principles of binary code, using combinations of ones and zeros to represent information. The evolution of computers has been remarkable, starting from room-sized mainframes to today's powerful and compact devices. Modern computers consist of hardware components like processors, memory, storage, and input/output devices, complemented by software that enables various applications and functionalities. The computer has become an integral part of daily life, influencing fields such as communication, entertainment, education, and research.\"\"\"\n",
        "\n",
        "doc2 = \"\"\"Artificial Intelligence, or AI, refers to the development of computer systems that can perform tasks that typically require human intelligence. This includes learning, reasoning, problem-solving, perception, and language understanding. Machine learning, a subset of AI, enables computers to improve their performance on a task through experience, without being explicitly programmed. AI applications range from virtual personal assistants and recommendation systems to complex tasks like image recognition, natural language processing, and autonomous vehicles. As AI continues to advance, ethical considerations and responsible development are crucial to ensure its positive impact on society.\"\"\"\n",
        "\n",
        "# Step 1: Preprocess the documents\n",
        "# (No explicit preprocessing needed for this example)\n",
        "\n",
        "# Step 2: Vectorize the documents\n",
        "vectorizer = CountVectorizer()\n",
        "vector = vectorizer.fit_transform([doc1, doc2])\n",
        "\n",
        "# Step 3: Find cosine similarity\n",
        "cos_sim = cosine_similarity(vector)\n",
        "\n",
        "print(\"Cosine Similarity Without Embedding:\")\n",
        "print(cos_sim)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Preprocess the documents\n",
        "# Tokenize the documents\n",
        "tokenized_doc1 = word_tokenize(doc1.lower())\n",
        "tokenized_doc2 = word_tokenize(doc2.lower())\n",
        "\n",
        "# Step 2: Use Word2Vec model for vector embedding\n",
        "model = Word2Vec([tokenized_doc1, tokenized_doc2], vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Step 3: Find the cosine similarity\n",
        "# Calculate the mean vector for each document\n",
        "vec_doc1 = np.mean([model.wv[word] for word in tokenized_doc1], axis=0)\n",
        "vec_doc2 = np.mean([model.wv[word] for word in tokenized_doc2], axis=0)\n",
        "\n",
        "# Reshape vectors for cosine similarity calculation\n",
        "vec_doc1 = vec_doc1.reshape(1, -1)\n",
        "vec_doc2 = vec_doc2.reshape(1, -1)\n",
        "\n",
        "# Calculate cosine similarity\n",
        "cos_sim_embedding = cosine_similarity(vec_doc1, vec_doc2)\n",
        "\n",
        "print(\"\\nCosine Similarity With Embedding:\")\n",
        "print(cos_sim_embedding)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPc3mn3w_6AY",
        "outputId": "fde0901d-ee8e-45f6-b0fe-99516a4e8c0d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Cosine Similarity With Embedding:\n",
            "[[0.7351834]]\n"
          ]
        }
      ]
    }
  ]
}